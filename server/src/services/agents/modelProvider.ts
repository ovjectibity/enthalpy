import Anthropic from "@anthropic-ai/sdk";
import {
  MessageParam,
  ContentBlockParam,
  ContentBlock,
  Tool as AnthropicTool
} from "@anthropic-ai/sdk/resources";
import { ModelMessage, AssistantModelMessage, CTInput } from "@enthalpy/shared";
import {
  modelMessageSchemaUpdated as modelMessageSchema,
  modelMessageZodSchema
} from "../../schemas/modelMessageSchemaUpdated.js";
import { prompts } from "../../schemas/mcprompts.js";
import util from 'util';

export interface LLMIntf {
  providerName: string,
  model: string;
  input: (msg: Array<ModelMessage>,tools?: Map<string,any>) => Promise<ModelMessage>
}

export class ClaudeIntf implements LLMIntf {
  providerName: string = "claude";
  // model: string = "claude-haiku-4-5-20251001";
  model: string = "claude-sonnet-4-5-20250929";
  maxTokens: number = 1024;
  client: Anthropic;

  constructor() {
    if (process.env.ANTHROPIC_KEY === undefined) {
      console.log("Set the env variable ANTHROPIC_KEY");
    }
    this.client = new Anthropic({
      apiKey: process.env.ANTHROPIC_KEY,
    });
  }

  //TODO: Cache Anthropic messages directly in this 
  // instance instead of constructing them everytime
  static translateToAnthropicMessage(msg: ModelMessage): MessageParam {
    let blocks = Array<ContentBlockParam>();
    if(msg.role === "assistant") {
      msg.contents.forEach(m => {
        if(m.type === "tool_use") {
          blocks.push({
            type: "tool_use",
            id: m.content.id,
            input: m.content.input,
            name: m.content.name
          })
        } else {
          blocks.push({
            type: "text",
            text: JSON.stringify(m)
          });
        }
      });
    } else if(msg.role === "user") {
      msg.contents.forEach(m => {
        if(m.type === "tool_use_result") {
          if(m.content.name === "computer_use") {
            blocks.push({
              type: "tool_result",
              tool_use_id: m.content.id,
              content: m.content.error && m.content.screengrab ? 
                JSON.stringify(m.content.error) : 
                [{
                  type: "image",
                  source: {
                    type: "base64",
                    media_type: "image/png",
                    data: m.content.screengrab ? m.content.screengrab : "" 
                  }
                }]
            });
          }
        } else {
          blocks.push({
            type: "text",
            text: JSON.stringify(m)
          });
        }
      });
    } 
    
    return {
      role: msg.role,
      content: blocks
    }
  }

  static translateToAnthropicMessages(msgs: Array<ModelMessage>):
    Array<MessageParam> {
    let anthMsgs = Array<MessageParam>();
    msgs.forEach((msg2: ModelMessage) => {
      // console.log("DEBUGGING: model message2: ",  JSON.stringify(msg2));
      anthMsgs.push(ClaudeIntf.translateToAnthropicMessage(msg2));
    });
    return anthMsgs;
  }

  static translateToAnthropicTool(toolSchemas: Map<string, any>):
    Array<AnthropicTool> {
    let anthTools = Array<AnthropicTool>();
    toolSchemas.forEach((name: string, toolSchema: any) => {
      // console.log("DEBUGGING: model message2: ",  JSON.stringify(msg2));
      anthTools.push({
        type: "custom",
        description: toolSchema.description,
        input_schema: toolSchema,
        name: name
      });
    });
    return anthTools;
  }

  async input_test(): Promise<ModelMessage> {
    return {
        role: "assistant",
        contents: [
          {
            type: "output_to_user",
            content: "Some content for the user generated by the agent."
          },
          {
            type: "workflow_context",
            content: "Some context gathered by the LLM"
          }
        ]
      };
  }

  async input(msgs: Array<ModelMessage>,toolSchemas?: Map<string,any>): Promise<ModelMessage> {
    // return this.input_test();
    console.log("Sending these input messages: ",msgs);
    // const message = await this.client.messages
    const message = await this.client.beta.messages
      .create({
        max_tokens: this.maxTokens,
        betas: ["structured-outputs-2025-11-13"],
        messages: ClaudeIntf.translateToAnthropicMessages(msgs),
        model: this.model,
        system: prompts["system-prompt"],
        tools: ClaudeIntf.translateToAnthropicTool(toolSchemas ? toolSchemas : new Map()),
        output_format: {
          type: "json_schema",
          schema: modelMessageSchema
        }
      })
      .catch((error) => {
        console.log(error);
      });
    // console.log("Got this message from the claude model", message);
    //TODO: translate the content blocks to ModelMessage format
    if (message === undefined || message === null) {
      return Promise.resolve({
        role: "assistant",
        contents: []
      });
    } else {
      let msg: AssistantModelMessage = {
        role: "assistant",
        contents: []
      };
      let contents = message.content as Array<ContentBlock>;
      contents.forEach((content: ContentBlock) => {
        if(content.type === "text") {
          if (content.text) {
            try {
              let modified = content.text.replace(/^```json|```$/g,"");
              let modifiedp: any = JSON.parse(modified);
              // Validate using Zod
              const validationResult = modelMessageZodSchema.safeParse(modifiedp);
              if(validationResult.success) {
                // console.log("Model output conforms to the schema, got this messages array: ",
                  // modifiedp.messages);
                let modifiedpv: AssistantModelMessage = modifiedp as AssistantModelMessage;
                msg.contents = msg.contents.concat(modifiedpv.contents);
              } else {
                console.log("Model output message does not conform to the schema");
                console.log("Validation errors:", validationResult.error);
                console.log("Received data:", util.inspect(modifiedp, { depth: null, colors: true }));
              }
            } catch(e) {
              console.log(`Parsing model output as JSON probably failed. Got LLM output ${content.text}`);
              console.log("Error when processing LLM response", e);
            }
          }
        } else if(content.type === "tool_use" && toolSchemas) {
          const toolId = content.id; 
          const toolName = content.name; 
          const input = content.input; 
          if(toolSchemas.has(toolName)) {
            if(toolName === "computer_use") {
              const cuInput = input as CTInput;
              msg.contents = msg.contents.concat([{
                type: "tool_use",
                content: {
                  name: "computer_use",
                  id: toolId,
                  input: cuInput
                }
              }]);
            }
          }
        }
      });
      return msg;
    }
  }
}
